{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58b20a0",
   "metadata": {},
   "source": [
    "# 1. 설정, 유틸 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13697f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas matplotlib seaborn scikit-learn mglearn pyarrow tensorflow keras tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 셀 1: 설정 + 유틸 =====\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ▼ CSV 파일이 있는 폴더와 파일명(원하면 수정)\n",
    "ROOT = \".\"\n",
    "FILES = [(\"ygs.csv\", \"YGS\"), (\"kum.csv\", \"KUM\"), (\"hws.csv\", \"HWS\"), (\"icn.csv\", \"ICN\")]\n",
    "\n",
    "# ▼ 출력 폴더\n",
    "OUTDIR = Path(\"preprocessed\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- CSV 읽기(자동 구분자 감지 + 컬럼명 정규화: 소문자/특수문자→_) ---\n",
    "def sniff_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[INFO] 파일 없음: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .str.replace(r\"[^0-9a-zA-Z_]+\", \"_\", regex=True)\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 읽기 실패 {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 데이터프레임 요약 출력 ---\n",
    "def print_df_summary(name: str, df: pd.DataFrame, head_rows: int = 3) -> None:\n",
    "    print(f\"\\n=== {name} | shape={df.shape} ===\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Sample rows:\")\n",
    "    print(df.head(head_rows).to_string(index=False))\n",
    "\n",
    "# --- 필수 컬럼 표준화(정확한 이름으로 강제) ---\n",
    "#   주신 실제 컬럼 목록에 맞춰 '그대로' 매핑합니다.\n",
    "#   - epc_code        ← 'epc_code'\n",
    "#   - event_time      ← 'event_time'\n",
    "#   - event_type      ← 'event_type'\n",
    "#   - location_id     ← 'location_id'  (이름 유지; category로 사용)\n",
    "REQUIRED = [\"epc_code\", \"event_time\", \"event_type\", \"location_id\"]\n",
    "\n",
    "def standardize_required(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 이미 소문자/언더스코어 처리된 상태라는 가정하에, 필수 컬럼 확인\n",
    "    missing = [c for c in REQUIRED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"필수 컬럼이 없습니다: {missing}\\n\"\n",
    "                    f\"현재 컬럼: {list(df.columns)}\")\n",
    "    # 불필요한 공백/형 변환 등은 아래 단계에서 처리\n",
    "    return df\n",
    "\n",
    "# --- 시간 파싱 + 시계열 피처(Δt, 시간대 sin/cos) ---\n",
    "def ensure_dt(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(series, errors=\"coerce\", utc=False)\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not {\"epc_code\", \"event_time\"}.issubset(df.columns):\n",
    "        raise KeyError(\"시간 피처 생성을 위해 'epc_code'와 'event_time'이 필요합니다.\")\n",
    "    out = df.copy()\n",
    "    out[\"event_time\"] = ensure_dt(out[\"event_time\"])\n",
    "    # EPC별 시간 정렬\n",
    "    out.sort_values([\"epc_code\", \"event_time\"], inplace=True, kind=\"stable\")\n",
    "    # Δt(초) — 시퀀스 시작은 0\n",
    "    out[\"delta_t_sec\"] = out.groupby(\"epc_code\")[\"event_time\"].diff().dt.total_seconds().fillna(0.0)\n",
    "    # 시간대(0~23) → sin/cos\n",
    "    hours = out[\"event_time\"].dt.hour.astype(float)\n",
    "    out[\"hour_sin\"] = (2 * math.pi * hours / 24.0).apply(math.sin)\n",
    "    out[\"hour_cos\"] = (2 * math.pi * hours / 24.0).apply(math.cos)\n",
    "    return out\n",
    "\n",
    "# --- 간단 어휘집 생성기 ---\n",
    "def build_vocab(series: pd.Series, name: str, min_freq: int = 1) -> Dict:\n",
    "    \"\"\"\n",
    "    - 예약 토큰: <PAD>=0, <UNK>=1\n",
    "    - 정렬: 빈도 내림차순 → (동률 시) 토큰 문자열 오름차순\n",
    "    - location_id는 숫자여도 '코드'이므로 문자열로 변환 후 카테고리로 취급합니다.\n",
    "    \"\"\"\n",
    "    s = series.astype(str)\n",
    "    counts = s.value_counts(dropna=False)\n",
    "    counts = counts[counts >= min_freq]\n",
    "    items = sorted(counts.items(), key=lambda kv: (-int(kv[1]), str(kv[0])))\n",
    "    id2token = [\"<PAD>\", \"<UNK>\"] + [str(k) for k, _ in items]\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"reserved\": [\"<PAD>\", \"<UNK>\"],\n",
    "        \"id2token\": id2token,\n",
    "        \"counts\": {str(k): int(v) for k, v in counts.items()},\n",
    "    }\n",
    "\n",
    "# --- 어휘집을 이용해 ID 인코딩 ---\n",
    "def encode_with_vocab(series: pd.Series, vocab: Dict, colname: str) -> pd.Series:\n",
    "    token2id = {tok: i for i, tok in enumerate(vocab[\"id2token\"])}\n",
    "    unk_id = vocab[\"reserved\"].index(\"<UNK>\") if \"<UNK>\" in vocab[\"reserved\"] else 1\n",
    "    return series.astype(str).map(lambda x: token2id.get(x, unk_id)).astype(\"int32\")\n",
    "\n",
    "# --- 저장 도우미(Parquet 우선, 실패 시 CSV 폴백) ---\n",
    "def save_table(df: pd.DataFrame, path_parquet: Path, path_csv: Optional[Path] = None):\n",
    "    try:\n",
    "        df.to_parquet(path_parquet, index=False)\n",
    "        print(f\"[OK] 저장(parquet): {path_parquet} (shape={df.shape})\")\n",
    "    except Exception as e:\n",
    "        if path_csv is None:\n",
    "            path_csv = path_parquet.with_suffix(\".csv\")\n",
    "        df.to_csv(path_csv, index=False)\n",
    "        print(f\"[WARN] parquet 저장 실패({e}) → CSV로 저장: {path_csv} (shape={df.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e417c8",
   "metadata": {},
   "source": [
    "# 2. CSV 읽기 & 요약 / 병합 → 필수 컬럼 표준화 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb24a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ygs.csv (YGS) | shape=(132200, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          양산공장            3 YGS_Factory       Factory Aggregation            3          3 001.8805843.2932031.100001.20250701.000000001           1      8805843      2932031   100001         20250701           1    Product 1 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          양산공장            3 YGS_Factory       Factory Aggregation            3          3 001.8809437.1203199.100002.20250701.000000002           1      8809437      1203199   100002         20250701           2    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          양산공장            3 YGS_Factory       Factory Aggregation            3          3 001.8809437.1203199.100002.20250701.000000003           1      8809437      1203199   100002         20250701           3    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "\n",
      "=== kum.csv (KUM) | shape=(129800, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          구미공장            4 KUM_Factory       Factory Aggregation            4          4 001.8805843.2932031.150001.20250701.000000001           1      8805843      2932031   150001         20250701           1    Product 1 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          구미공장            4 KUM_Factory       Factory Aggregation            4          4 001.8809437.1203199.150002.20250701.000000002           1      8809437      1203199   150002         20250701           2    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          구미공장            4 KUM_Factory       Factory Aggregation            4          4 001.8809437.1203199.150002.20250701.000000003           1      8809437      1203199   150002         20250701           3    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "\n",
      "=== hws.csv (HWS) | shape=(220800, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          화성공장            2 HWS_Factory       Factory Aggregation            2          2 001.8805843.2932031.050001.20250701.000000001           1      8805843      2932031    50001         20250701           1    Product 1 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          화성공장            2 HWS_Factory       Factory Aggregation            2          2 001.8809437.1203199.050002.20250701.000000002           1      8809437      1203199    50002         20250701           2    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          화성공장            2 HWS_Factory       Factory Aggregation            2          2 001.8809437.1203199.050002.20250701.000000003           1      8809437      1203199    50002         20250701           3    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "\n",
      "=== icn.csv (ICN) | shape=(437200, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          인천공장            1 ICN_Factory       Factory Aggregation            1          1 001.8805843.2932031.010001.20250701.000000001           1      8805843      2932031    10001         20250701           1    Product 1 2025-07-01 10:23:38 2025-07-01 10:23:38     20251231\n",
      "          인천공장            1 ICN_Factory       Factory Aggregation            1          1 001.8809437.1203199.010002.20250701.000000002           1      8809437      1203199    10002         20250701           2    Product 2 2025-07-01 10:23:38 2025-07-01 10:23:38     20251231\n",
      "          인천공장            1 ICN_Factory       Factory Aggregation            1          1 001.8809437.1203199.010002.20250701.000000003           1      8809437      1203199    10002         20250701           3    Product 2 2025-07-01 10:23:38 2025-07-01 10:23:38     20251231\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 2: CSV 읽기 & 요약 =====\n",
    "import pandas as pd\n",
    "print(\"[INFO] 모든 CSV 파일을 읽어 하나의 테이블로 병합합니다...\")\n",
    "\n",
    "dfs = []\n",
    "for fname, factory in FILES:\n",
    "    path = os.path.join(ROOT, fname)\n",
    "    df = sniff_read_csv(path)\n",
    "    if df is None:\n",
    "        continue\n",
    "    print_df_summary(f\"{fname} ({factory})\", df)\n",
    "    # factory가 데이터에 별도 컬럼으로 있을 수도 있지만, 파일별 라벨을 임시로 붙여둡니다.\n",
    "    df[\"__factory_label__\"] = factory\n",
    "    dfs.append(df)\n",
    "\n",
    "if not dfs:\n",
    "    raise SystemExit(\"[STOP] CSV를 하나도 찾지 못했습니다. 파일 경로/이름을 확인하세요.\")\n",
    "\n",
    "# ===== 병합/표준화/저장 =====\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\n[INFO] 병합 shape: {merged.shape}\")\n",
    "\n",
    "# 필수 컬럼 확인/고정(정확한 이름 사용)\n",
    "std = standardize_required(merged)\n",
    "\n",
    "# factory 최종화: 원본에 'factory'가 없다면, 임시 라벨 사용\n",
    "if \"factory\" not in std.columns:\n",
    "    std[\"factory\"] = std.pop(\"__factory_label__\")\n",
    "else:\n",
    "    std.drop(columns=[\"__factory_label__\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "# [수정 제안] Parquet 저장을 위해 데이터 타입을 명시적으로 변환\n",
    "# 문제가 될 만한 object 타입 컬럼을 문자열(str)로 바꿔줍니다.\n",
    "for col in std.select_dtypes(include=['object']).columns:\n",
    "    print(f\"[DEBUG] Converting column '{col}' to string type for Parquet compatibility.\")\n",
    "    std[col] = std[col].astype(str)\n",
    "\n",
    "# 저장\n",
    "save_table(std, OUTDIR / \"combined.parquet\")\n",
    "print(\"[OK] 데이터 병합 및 기본 표준화 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddab5b",
   "metadata": {},
   "source": [
    "# 3. 시간 피처(Δt, hour sin/cos) 생성 → 저장 / 어휘집 생성(JSON) + ID 인코딩 컬럼 추가 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97885891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 저장(parquet): preprocessed/combined_with_time.parquet (shape=(920000, 22))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# ===== 셀 3: 시간 피처 생성/저장 =====\n",
    "\n",
    "print(\"\\n[INFO] 시간 피처 생성 및 ID 인코딩을 시작합니다...\")\n",
    "std = pd.read_parquet(OUTDIR / \"combined.parquet\") # 앞 단계 결과물 로드\n",
    "std_time = add_time_features(std)\n",
    "\n",
    "# --- [수정] Parquet 저장을 위한 임시 데이터프레임 생성 ---\n",
    "# 원본 std_time의 타입을 바꾸지 않고, 저장을 위한 복사본을 만듭니다.\n",
    "df_to_save = std_time.copy()\n",
    "\n",
    "# datetime 타입은 종종 Parquet 변환 시 문제를 일으키므로,\n",
    "# 가장 안전한 ISO 8601 표준 문자열 포맷으로 변환하여 저장합니다.\n",
    "df_to_save[\"event_time\"] = df_to_save[\"event_time\"].astype(str)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "save_table(std_time, OUTDIR / \"combined_with_time.parquet\")\n",
    "\n",
    "\n",
    "# ===== vocab 생성 + ID 인코딩 =====\n",
    "# 1) event_type 어휘집\n",
    "vocab_event = build_vocab(std_time[\"event_type\"], name=\"event_type\", min_freq=1)\n",
    "with open(OUTDIR / \"event_type.vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_event, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] 어휘집 저장: {OUTDIR / 'event_type.vocab.json'} (size={len(vocab_event['id2token'])})\")\n",
    "\n",
    "# 2) location_id 어휘집 (숫자지만 '코드'이므로 카테고리 취급)\n",
    "vocab_loc = build_vocab(std_time[\"location_id\"], name=\"location_id\", min_freq=1)\n",
    "with open(OUTDIR / \"location_id.vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_loc, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] 어휘집 저장: {OUTDIR / 'location_id.vocab.json'} (size={len(vocab_loc['id2token'])})\")\n",
    "\n",
    "# 3) ID 인코딩 컬럼 추가\n",
    "encoded = std_time.copy()\n",
    "encoded[\"event_type_id\"]  = encode_with_vocab(encoded[\"event_type\"],  vocab_event, \"event_type\")\n",
    "encoded[\"location_id_id\"] = encode_with_vocab(encoded[\"location_id\"], vocab_loc,   \"location_id\")\n",
    "\n",
    "# 저장\n",
    "save_table(encoded, OUTDIR / \"combined_encoded.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473500b",
   "metadata": {},
   "source": [
    "# 4. 모델 학습을 위한 준비 - 데이터 분할 및 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 스케일링을 위해 인코딩된 데이터를 불러옵니다.\n",
      "[INFO] 데이터 분할 완료: Train=(736188, 24), Test=(183812, 24)\n",
      "[INFO] 학습 데이터의 '['delta_t_sec']' 컬럼에 스케일러를 학습(fit)합니다.\n",
      "[INFO] 학습된 스케일러로 Train/Test 데이터를 변환(transform)합니다.\n",
      "[OK] 학습된 스케일러 저장 완료: preprocessed/delta_t_scaler.joblib\n",
      "[OK] 저장(parquet): preprocessed/train_final.parquet (shape=(736188, 24))\n",
      "[OK] 저장(parquet): preprocessed/test_final.parquet (shape=(183812, 24))\n",
      "\n",
      "--- 스케일링 결과 확인 (Train Set) ---\n",
      "        delta_t_sec\n",
      "count  7.361880e+05\n",
      "mean  -1.482493e-17\n",
      "std    1.000001e+00\n",
      "min   -5.533837e-01\n",
      "25%   -5.508160e-01\n",
      "50%   -5.353657e-01\n",
      "75%    6.948806e-02\n",
      "max    3.022292e+00\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 4: 데이터 분할, 스케일링, 최종 저장 =====\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib # 스케일러 저장을 위해 사용\n",
    "\n",
    "print(\"\\n[INFO] 최종 피처 엔지니어링 및 데이터 분할/스케일링을 시작합니다...\")\n",
    "\n",
    "# 1. 셀 5에서 최종적으로 생성된 데이터를 불러옵니다.\n",
    "print(\"[INFO] 스케일링을 위해 인코딩된 데이터를 불러옵니다.\")\n",
    "try:\n",
    "    final_df = pd.read_parquet(OUTDIR / \"combined_encoded.parquet\")\n",
    "except FileNotFoundError:\n",
    "    # Parquet 저장 실패 시 생성된 CSV를 대신 읽음\n",
    "    final_df = pd.read_csv(OUTDIR / \"combined_encoded.csv\")\n",
    "\n",
    "# 2. 데이터를 학습(Train)과 테스트(Test) 세트로 분할합니다.\n",
    "#    (시계열 데이터이므로 epc_code 단위로 분할하거나, 시간순으로 분할하는 것이 더 좋습니다.)\n",
    "#    여기서는 간단한 예시로 랜덤 분할을 사용합니다. epc_code를 기준으로 분할해야 합니다.\n",
    "epc_codes = final_df[\"epc_code\"].unique()\n",
    "\n",
    "# 2. 이 식별자 자체를 80:20으로 랜덤하게 나눕니다.\n",
    "train_epcs, test_epcs = train_test_split(epc_codes, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = final_df[final_df[\"epc_code\"].isin(train_epcs)].copy()\n",
    "test_df = final_df[final_df[\"epc_code\"].isin(test_epcs)].copy()\n",
    "\n",
    "print(f\"[INFO] 데이터 분할 완료: Train={train_df.shape}, Test={test_df.shape}\")\n",
    "\n",
    "# 3. 스케일링 할 컬럼을 지정합니다.\n",
    "cols_to_scale = ['delta_t_sec'] # 스케일 조정이 필요한 컬럼 목록\n",
    "\n",
    "# 4. StandardScaler를 생성하고 '학습 데이터'에만 fit합니다.\n",
    "scaler = StandardScaler()\n",
    "print(f\"[INFO] 학습 데이터의 '{cols_to_scale}' 컬럼에 스케일러를 학습(fit)합니다.\")\n",
    "# scaler.fit()은 2D 배열을 기대하므로, 대괄호를 두 번 사용합니다.\n",
    "scaler.fit(train_df[cols_to_scale])\n",
    "\n",
    "# 5. 학습된 스케일러로 Train, Test 데이터 모두를 transform합니다.\n",
    "print(\"[INFO] 학습된 스케일러로 Train/Test 데이터를 변환(transform)합니다.\")\n",
    "train_df[cols_to_scale] = scaler.transform(train_df[cols_to_scale])\n",
    "test_df[cols_to_scale] = scaler.transform(test_df[cols_to_scale])\n",
    "\n",
    "# 6. 학습된 스케일러를 파일로 저장합니다. (어휘집 저장만큼 중요!)\n",
    "scaler_path = OUTDIR / \"delta_t_scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"[OK] 학습된 스케일러 저장 완료: {scaler_path}\")\n",
    "\n",
    "# 7. 스케일링까지 완료된 최종 데이터셋을 각각 저장합니다.\n",
    "save_table(train_df, OUTDIR / \"train_final.parquet\")\n",
    "save_table(test_df, OUTDIR / \"test_final.parquet\")\n",
    "\n",
    "print(\"\\n--- 스케일링 결과 확인 (Train Set) ---\")\n",
    "print(train_df[cols_to_scale].describe())\n",
    "\n",
    "print(\"[OK] 전처리 최종 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
