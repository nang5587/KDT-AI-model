{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58b20a0",
   "metadata": {},
   "source": [
    "# 0. 기존 DB자료 \"Tab\" → \",\" 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1699397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 16:07:13.910653: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-20 16:07:13.933990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-20 16:07:25.545142: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TensorFlow GPU 확인을 시작합니다...\n",
      "[INFO] 발견된 물리적 GPU: 1개\n",
      "[SUCCESS] GPU 메모리 동적 할당 설정에 성공했습니다!\n",
      "[FINAL CHECK] TensorFlow가 인식하고 사용할 수 있는 GPU: 1개\n",
      "       >>> GPU가 성공적으로 인식되었습니다. 이제부터 GPU 연산이 가능합니다. <<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755673650.389777  377818 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21055 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#                  ★★ 최종 GPU 설정 및 라이브러리 임포트 셀 ★★\n",
    "#               (노트북을 열면 무조건 이 셀부터 실행하세요)\n",
    "# ==============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def initialize_gpu():\n",
    "    \"\"\"\n",
    "    TensorFlow GPU를 안전하게 초기화하고, 이미 초기화된 경우에도 정보를 제공합니다.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] TensorFlow GPU 확인을 시작합니다...\")\n",
    "    \n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    \n",
    "    if not gpus:\n",
    "        print(\"[WARN] GPU를 찾을 수 없습니다. CPU로 실행됩니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] 발견된 물리적 GPU: {len(gpus)}개\")\n",
    "\n",
    "    try:\n",
    "        # 메모리 동적 할당을 '시도'합니다.\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"[SUCCESS] GPU 메모리 동적 할당 설정에 성공했습니다!\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        # 만약 오류가 발생하면 (대부분 이미 초기화된 경우)\n",
    "        print(f\"[NOTE] GPU 메모리 설정 중 참고사항: {e}\")\n",
    "        print(\"       (VS Code의 자동 실행 등으로 이미 초기화되었을 수 있으나, GPU 사용에는 문제가 없을 가능성이 높습니다.)\")\n",
    "\n",
    "    # 최종적으로 TensorFlow가 실제로 사용할 수 있는 논리적 GPU 개수를 확인합니다.\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(f\"[FINAL CHECK] TensorFlow가 인식하고 사용할 수 있는 GPU: {len(logical_gpus)}개\")\n",
    "    \n",
    "    if len(logical_gpus) > 0:\n",
    "        print(\"       >>> GPU가 성공적으로 인식되었습니다. 이제부터 GPU 연산이 가능합니다. <<<\")\n",
    "    else:\n",
    "        print(\"       >>> [ERROR] GPU가 발견되었으나, TensorFlow가 사용할 수 없는 상태입니다. 드라이버나 설치를 확인하세요. <<<\")\n",
    "\n",
    "# 함수를 호출하여 GPU 설정을 실행합니다.\n",
    "initialize_gpu()\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13697f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas matplotlib seaborn scikit-learn mglearn pyarrow tensorflow keras tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d17fef",
   "metadata": {},
   "source": [
    "# 1. 설정, 유틸 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 셀 1: 설정 + 유틸 =====\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# ▼ CSV 파일이 있는 폴더와 파일명(원하면 수정)\n",
    "ROOT = \".\"\n",
    "FILES = [(\"ygs.csv\", \"YGS\"), (\"kum.csv\", \"KUM\"), (\"hws.csv\", \"HWS\"), (\"icn.csv\", \"ICN\")]\n",
    "\n",
    "# ▼ 출력 폴더\n",
    "OUTDIR = Path(\"preprocessed\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- CSV 읽기(자동 구분자 감지 + 컬럼명 정규화: 소문자/특수문자→_) ---\n",
    "def sniff_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[INFO] 파일 없음: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "        df.columns = (\n",
    "            df.columns\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .str.replace(r\"[^0-9a-zA-Z_]+\", \"_\", regex=True)\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 읽기 실패 {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 데이터프레임 요약 출력 ---\n",
    "def print_df_summary(name: str, df: pd.DataFrame, head_rows: int = 3) -> None:\n",
    "    print(f\"\\n=== {name} | shape={df.shape} ===\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Sample rows:\")\n",
    "    print(df.head(head_rows).to_string(index=False))\n",
    "\n",
    "# --- 필수 컬럼 표준화(정확한 이름으로 강제) ---\n",
    "#   주신 실제 컬럼 목록에 맞춰 '그대로' 매핑합니다.\n",
    "#   - epc_code        ← 'epc_code'\n",
    "#   - event_time      ← 'event_time'\n",
    "#   - event_type      ← 'event_type'\n",
    "#   - location_id     ← 'location_id'  (이름 유지; category로 사용)\n",
    "REQUIRED = [\"epc_code\", \"event_time\", \"event_type\", \"location_id\"]\n",
    "\n",
    "def standardize_required(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 이미 소문자/언더스코어 처리된 상태라는 가정하에, 필수 컬럼 확인\n",
    "    missing = [c for c in REQUIRED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"필수 컬럼이 없습니다: {missing}\\n\"\n",
    "                    f\"현재 컬럼: {list(df.columns)}\")\n",
    "    # 불필요한 공백/형 변환 등은 아래 단계에서 처리\n",
    "    return df\n",
    "\n",
    "# --- 시간 파싱 + 시계열 피처(Δt, 시간대 sin/cos) ---\n",
    "def ensure_dt(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(series, errors=\"coerce\", utc=False)\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not {\"epc_code\", \"event_time\"}.issubset(df.columns):\n",
    "        raise KeyError(\"시간 피처 생성을 위해 'epc_code'와 'event_time'이 필요합니다.\")\n",
    "    out = df.copy()\n",
    "    out[\"event_time\"] = ensure_dt(out[\"event_time\"])\n",
    "    # EPC별 시간 정렬\n",
    "    out.sort_values([\"epc_code\", \"event_time\"], inplace=True, kind=\"stable\")\n",
    "    # Δt(초) — 시퀀스 시작은 0\n",
    "    out[\"delta_t_sec\"] = out.groupby(\"epc_code\")[\"event_time\"].diff().dt.total_seconds().fillna(0.0)\n",
    "    # 시간대(0~23) → sin/cos\n",
    "    hours = out[\"event_time\"].dt.hour.astype(float)\n",
    "    out[\"hour_sin\"] = (2 * math.pi * hours / 24.0).apply(math.sin)\n",
    "    out[\"hour_cos\"] = (2 * math.pi * hours / 24.0).apply(math.cos)\n",
    "    return out\n",
    "\n",
    "# --- 간단 어휘집 생성기 ---\n",
    "def build_vocab(series: pd.Series, name: str, min_freq: int = 1) -> Dict:\n",
    "    \"\"\"\n",
    "    - 예약 토큰: <PAD>=0, <UNK>=1\n",
    "    - 정렬: 빈도 내림차순 → (동률 시) 토큰 문자열 오름차순\n",
    "    - location_id는 숫자여도 '코드'이므로 문자열로 변환 후 카테고리로 취급합니다.\n",
    "    \"\"\"\n",
    "    s = series.astype(str)\n",
    "    counts = s.value_counts(dropna=False)\n",
    "    counts = counts[counts >= min_freq]\n",
    "    items = sorted(counts.items(), key=lambda kv: (-int(kv[1]), str(kv[0])))\n",
    "    id2token = [\"<PAD>\", \"<UNK>\"] + [str(k) for k, _ in items]\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"reserved\": [\"<PAD>\", \"<UNK>\"],\n",
    "        \"id2token\": id2token,\n",
    "        \"counts\": {str(k): int(v) for k, v in counts.items()},\n",
    "    }\n",
    "\n",
    "# --- 어휘집을 이용해 ID 인코딩 ---\n",
    "def encode_with_vocab(series: pd.Series, vocab: Dict, colname: str) -> pd.Series:\n",
    "    token2id = {tok: i for i, tok in enumerate(vocab[\"id2token\"])}\n",
    "    unk_id = vocab[\"reserved\"].index(\"<UNK>\") if \"<UNK>\" in vocab[\"reserved\"] else 1\n",
    "    return series.astype(str).map(lambda x: token2id.get(x, unk_id)).astype(\"int32\")\n",
    "\n",
    "# --- 저장 도우미(Parquet 우선, 실패 시 CSV 폴백) ---\n",
    "def save_table(df: pd.DataFrame, path_parquet: Path, path_csv: Optional[Path] = None):\n",
    "    try:\n",
    "        df.to_parquet(path_parquet, index=False)\n",
    "        print(f\"[OK] 저장(parquet): {path_parquet} (shape={df.shape})\")\n",
    "    except Exception as e:\n",
    "        if path_csv is None:\n",
    "            path_csv = path_parquet.with_suffix(\".csv\")\n",
    "        df.to_csv(path_csv, index=False)\n",
    "        print(f\"[WARN] parquet 저장 실패({e}) → CSV로 저장: {path_csv} (shape={df.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e417c8",
   "metadata": {},
   "source": [
    "# 2. CSV 읽기 & 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb24a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ygs.csv (YGS) | shape=(132200, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          양산공장            3 YGS_Factory       Factory Aggregation            3          3 001.8805843.2932031.100001.20250701.000000001           1      8805843      2932031   100001         20250701           1    Product 1 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          양산공장            3 YGS_Factory       Factory Aggregation            3          3 001.8809437.1203199.100002.20250701.000000002           1      8809437      1203199   100002         20250701           2    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          양산공장            3 YGS_Factory       Factory Aggregation            3          3 001.8809437.1203199.100002.20250701.000000003           1      8809437      1203199   100002         20250701           3    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "\n",
      "=== kum.csv (KUM) | shape=(129800, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          구미공장            4 KUM_Factory       Factory Aggregation            4          4 001.8805843.2932031.150001.20250701.000000001           1      8805843      2932031   150001         20250701           1    Product 1 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          구미공장            4 KUM_Factory       Factory Aggregation            4          4 001.8809437.1203199.150002.20250701.000000002           1      8809437      1203199   150002         20250701           2    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          구미공장            4 KUM_Factory       Factory Aggregation            4          4 001.8809437.1203199.150002.20250701.000000003           1      8809437      1203199   150002         20250701           3    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "\n",
      "=== hws.csv (HWS) | shape=(220800, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          화성공장            2 HWS_Factory       Factory Aggregation            2          2 001.8805843.2932031.050001.20250701.000000001           1      8805843      2932031    50001         20250701           1    Product 1 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          화성공장            2 HWS_Factory       Factory Aggregation            2          2 001.8809437.1203199.050002.20250701.000000002           1      8809437      1203199    50002         20250701           2    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "          화성공장            2 HWS_Factory       Factory Aggregation            2          2 001.8809437.1203199.050002.20250701.000000003           1      8809437      1203199    50002         20250701           3    Product 2 2025-07-01 10:23:39 2025-07-01 10:23:39     20251231\n",
      "\n",
      "=== icn.csv (ICN) | shape=(437200, 18) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date']\n",
      "Sample rows:\n",
      "_scan_location  location_id    hub_type business_step  event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date\n",
      "          인천공장            1 ICN_Factory       Factory Aggregation            1          1 001.8805843.2932031.010001.20250701.000000001           1      8805843      2932031    10001         20250701           1    Product 1 2025-07-01 10:23:38 2025-07-01 10:23:38     20251231\n",
      "          인천공장            1 ICN_Factory       Factory Aggregation            1          1 001.8809437.1203199.010002.20250701.000000002           1      8809437      1203199    10002         20250701           2    Product 2 2025-07-01 10:23:38 2025-07-01 10:23:38     20251231\n",
      "          인천공장            1 ICN_Factory       Factory Aggregation            1          1 001.8809437.1203199.010002.20250701.000000003           1      8809437      1203199    10002         20250701           3    Product 2 2025-07-01 10:23:38 2025-07-01 10:23:38     20251231\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 2: CSV 읽기 & 요약 =====\n",
    "dfs = []\n",
    "for fname, factory in FILES:\n",
    "    path = os.path.join(ROOT, fname)\n",
    "    df = sniff_read_csv(path)\n",
    "    if df is None:\n",
    "        continue\n",
    "    print_df_summary(f\"{fname} ({factory})\", df)\n",
    "    # factory가 데이터에 별도 컬럼으로 있을 수도 있지만, 파일별 라벨을 임시로 붙여둡니다.\n",
    "    df[\"__factory_label__\"] = factory\n",
    "    dfs.append(df)\n",
    "\n",
    "if not dfs:\n",
    "    raise SystemExit(\"[STOP] CSV를 하나도 찾지 못했습니다. 파일 경로/이름을 확인하세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac377d1",
   "metadata": {},
   "source": [
    "# 3. 병합 → 필수 컬럼 표준화 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5009f215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 병합 shape: (920000, 19)\n",
      "[DEBUG] Converting column '_scan_location' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'hub_type' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'business_step' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'event_type' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'epc_code' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'product_name' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'event_time' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'manufacture_date' to string type for Parquet compatibility.\n",
      "[DEBUG] Converting column 'factory' to string type for Parquet compatibility.\n",
      "[OK] 저장(parquet): preprocessed/combined.parquet (shape=(920000, 19))\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 3: 병합/표준화/저장 =====\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\n[INFO] 병합 shape: {merged.shape}\")\n",
    "\n",
    "# 필수 컬럼 확인/고정(정확한 이름 사용)\n",
    "std = standardize_required(merged)\n",
    "\n",
    "# factory 최종화: 원본에 'factory'가 없다면, 임시 라벨 사용\n",
    "if \"factory\" not in std.columns:\n",
    "    std[\"factory\"] = std.pop(\"__factory_label__\")\n",
    "else:\n",
    "    std.drop(columns=[\"__factory_label__\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "# [수정 제안] Parquet 저장을 위해 데이터 타입을 명시적으로 변환\n",
    "# 문제가 될 만한 object 타입 컬럼을 문자열(str)로 바꿔줍니다.\n",
    "for col in std.select_dtypes(include=['object']).columns:\n",
    "    print(f\"[DEBUG] Converting column '{col}' to string type for Parquet compatibility.\")\n",
    "    std[col] = std[col].astype(str)\n",
    "\n",
    "# 저장\n",
    "save_table(std, OUTDIR / \"combined.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddab5b",
   "metadata": {},
   "source": [
    "# 4. 시간 피처(Δt, hour sin/cos) 생성 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97885891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 저장(parquet): preprocessed/combined_with_time.parquet (shape=(920000, 22))\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 4: 시간 피처 생성/저장 =====\n",
    "std_time = add_time_features(std)\n",
    "\n",
    "# --- [수정] Parquet 저장을 위한 임시 데이터프레임 생성 ---\n",
    "# 원본 std_time의 타입을 바꾸지 않고, 저장을 위한 복사본을 만듭니다.\n",
    "df_to_save = std_time.copy()\n",
    "\n",
    "# datetime 타입은 종종 Parquet 변환 시 문제를 일으키므로,\n",
    "# 가장 안전한 ISO 8601 표준 문자열 포맷으로 변환하여 저장합니다.\n",
    "df_to_save[\"event_time\"] = df_to_save[\"event_time\"].astype(str)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "save_table(std_time, OUTDIR / \"combined_with_time.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3c05f",
   "metadata": {},
   "source": [
    "# 5. 어휘집 생성(JSON) + ID 인코딩 컬럼 추가 → 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4037a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 어휘집 저장: preprocessed/event_type.vocab.json (size=12)\n",
      "[OK] 어휘집 저장: preprocessed/location_id.vocab.json (size=60)\n",
      "[OK] 저장(parquet): preprocessed/combined_encoded.parquet (shape=(920000, 24))\n"
     ]
    }
   ],
   "source": [
    "# ===== 셀 5: vocab 생성 + ID 인코딩 =====\n",
    "\n",
    "# 1) event_type 어휘집\n",
    "vocab_event = build_vocab(std_time[\"event_type\"], name=\"event_type\", min_freq=1)\n",
    "with open(OUTDIR / \"event_type.vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_event, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] 어휘집 저장: {OUTDIR / 'event_type.vocab.json'} (size={len(vocab_event['id2token'])})\")\n",
    "\n",
    "# 2) location_id 어휘집 (숫자지만 '코드'이므로 카테고리 취급)\n",
    "vocab_loc = build_vocab(std_time[\"location_id\"], name=\"location_id\", min_freq=1)\n",
    "with open(OUTDIR / \"location_id.vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_loc, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] 어휘집 저장: {OUTDIR / 'location_id.vocab.json'} (size={len(vocab_loc['id2token'])})\")\n",
    "\n",
    "# 3) ID 인코딩 컬럼 추가\n",
    "encoded = std_time.copy()\n",
    "encoded[\"event_type_id\"]  = encode_with_vocab(encoded[\"event_type\"],  vocab_event, \"event_type\")\n",
    "encoded[\"location_id_id\"] = encode_with_vocab(encoded[\"location_id\"], vocab_loc,   \"location_id\")\n",
    "\n",
    "# 저장\n",
    "save_table(encoded, OUTDIR / \"combined_encoded.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473500b",
   "metadata": {},
   "source": [
    "# 6. 모델 학습을 위한 준비 - 데이터 분할 및 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2f3dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 인코딩된 데이터를 불러옵니다.\n",
      "\n",
      "[INFO] EPC 그룹 단위의 통계적 피처를 생성합니다...\n",
      "[OK] 그룹 통계 피처 병합 완료.\n",
      "\n",
      "=== Engineered DataFrame | shape=(5, 29) ===\n",
      "Columns: ['_scan_location', 'location_id', 'hub_type', 'business_step', 'event_type', 'operator_id', 'device_id', 'epc_code', 'epc_header', 'epc_company', 'epc_product', 'epc_lot', 'epc_manufacture', 'epc_serial', 'product_name', 'event_time', 'manufacture_date', 'expiry_date', 'factory', 'delta_t_sec', 'hour_sin', 'hour_cos', 'event_type_id', 'location_id_id', 'seq_length', 'n_unique_locations', 'delta_t_mean', 'delta_t_std', 'delta_t_max']\n",
      "Sample rows:\n",
      "_scan_location  location_id         hub_type business_step   event_type  operator_id  device_id                                      epc_code  epc_header  epc_company  epc_product  epc_lot  epc_manufacture  epc_serial product_name          event_time    manufacture_date  expiry_date factory  delta_t_sec  hour_sin  hour_cos  event_type_id  location_id_id  seq_length  n_unique_locations  delta_t_mean  delta_t_std  delta_t_max\n",
      "          인천공장            1      ICN_Factory       Factory  Aggregation            1          1 001.8804823.1293291.010004.20250701.000004002           1      8804823      1293291    10004         20250701        4002    Product 4 2025-07-01 10:34:17 2025-07-01 10:34:17     20251231     ICN          0.0  0.500000 -0.866025              2               3          10                   5     1018856.1 1.260946e+06    2939700.0\n",
      "        인천공장창고            5  ICN_WMS_Inbound           WMS  WMS_Inbound            5          5 001.8804823.1293291.010004.20250701.000004002           1      8804823      1293291    10004         20250701        4002    Product 4 2025-07-01 11:02:38 2025-07-01 10:34:17     20251231     ICN       1701.0  0.258819 -0.965926              3               2          10                   5     1018856.1 1.260946e+06    2939700.0\n",
      "        인천공장창고            5 ICN_WMS_Outbound           WMS WMS_Outbound            5          5 001.8804823.1293291.010004.20250701.000004002           1      8804823      1293291    10004         20250701        4002    Product 4 2025-07-29 16:57:38 2025-07-01 10:34:17     20251231     ICN    2440500.0 -0.866025 -0.500000              5               2          10                   5     1018856.1 1.260946e+06    2939700.0\n",
      "\n",
      "[INFO] 데이터 분할 완료: Train=(736188, 29), Test=(183812, 29)\n",
      "[INFO] 스케일링 대상 컬럼: ['delta_t_sec', 'seq_length', 'n_unique_locations', 'delta_t_mean', 'delta_t_std', 'delta_t_max']\n",
      "[INFO] 학습 데이터에 스케일러를 학습(fit)합니다.\n",
      "[INFO] 학습된 스케일러로 Train/Test 데이터를 변환(transform)합니다.\n",
      "[OK] 스케일러 저장 완료: preprocessed/if_scaler.joblib\n",
      "[OK] 저장(parquet): preprocessed/train_final_engineered.parquet (shape=(736188, 29))\n",
      "[OK] 저장(parquet): preprocessed/test_final_engineered.parquet (shape=(183812, 29))\n",
      "\n",
      "--- 스케일링 결과 확인 (Train Set) ---\n",
      "        delta_t_sec    seq_length  n_unique_locations  delta_t_mean  \\\n",
      "count  7.361880e+05  7.361880e+05        7.361880e+05  7.361880e+05   \n",
      "mean  -1.482493e-17 -6.918301e-17        6.918301e-17 -1.976657e-17   \n",
      "std    1.000001e+00  1.000001e+00        1.000001e+00  1.000001e+00   \n",
      "min   -5.533837e-01 -1.556013e+00       -1.650000e+00 -1.625292e+00   \n",
      "25%   -5.508160e-01 -7.783785e-01       -7.778331e-01 -7.550239e-01   \n",
      "50%   -5.353657e-01 -7.436347e-04        9.433341e-02  1.240440e-01   \n",
      "75%    6.948806e-02  7.768912e-01        9.664999e-01  8.157125e-01   \n",
      "max    3.022292e+00  1.554526e+00        9.664999e-01  3.186152e+00   \n",
      "\n",
      "        delta_t_std   delta_t_max  \n",
      "count  7.361880e+05  7.361880e+05  \n",
      "mean  -2.470822e-16 -5.929972e-17  \n",
      "std    1.000001e+00  1.000001e+00  \n",
      "min   -1.742038e+00 -1.746954e+00  \n",
      "25%   -5.967446e-01 -7.324173e-01  \n",
      "50%    2.250656e-01  2.760499e-01  \n",
      "75%    7.300292e-01  8.868805e-01  \n",
      "max    1.704259e+00  1.260141e+00  \n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#      (수정된 최종 셀 6) 피처 엔지니어링 + 데이터 분할 + 스케일링\n",
    "# ==============================================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# --- 1. 셀 5에서 최종적으로 생성된 데이터를 불러옵니다. ---\n",
    "print(\"[INFO] 인코딩된 데이터를 불러옵니다.\")\n",
    "try:\n",
    "    df_base = pd.read_parquet(OUTDIR / \"combined_encoded.parquet\")\n",
    "except Exception as e:\n",
    "    df_base = pd.read_csv(OUTDIR / \"combined_encoded.csv\")\n",
    "    print(f\"[WARN] Parquet 읽기 실패({e}), CSV 파일 사용.\")\n",
    "\n",
    "# --- 2. [신규] EPC 그룹 단위의 통계적 피처를 생성합니다. (Isolation Forest용) ---\n",
    "print(\"\\n[INFO] EPC 그룹 단위의 통계적 피처를 생성합니다...\")\n",
    "epc_group_stats = df_base.groupby('epc_code').agg(\n",
    "    seq_length=('event_time', 'count'),\n",
    "    n_unique_locations=('location_id', 'nunique'),\n",
    "    delta_t_mean=('delta_t_sec', 'mean'),\n",
    "    delta_t_std=('delta_t_sec', 'std'),\n",
    "    delta_t_max=('delta_t_sec', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# std가 없는 경우(시퀀스 길이가 1) NaN이 되므로 0으로 채웁니다.\n",
    "epc_group_stats.fillna(0, inplace=True)\n",
    "\n",
    "# 원본 데이터에 통계 피처를 병합하여 '최종 엔지니어링된 데이터'를 만듭니다.\n",
    "df_engineered = pd.merge(df_base, epc_group_stats, on='epc_code', how='left')\n",
    "\n",
    "print(\"[OK] 그룹 통계 피처 병합 완료.\")\n",
    "print_df_summary(\"Engineered DataFrame\", df_engineered.head()) # .head()로 요약본만 출력\n",
    "\n",
    "\n",
    "# --- 3. EPC 코드를 기준으로 데이터를 학습(Train)과 테스트(Test) 세트로 분할합니다. ---\n",
    "# (df_engineered를 분할해야 모든 피처가 train/test에 포함됩니다.)\n",
    "epc_codes = df_engineered[\"epc_code\"].unique()\n",
    "train_epcs, test_epcs = train_test_split(epc_codes, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df_engineered[df_engineered[\"epc_code\"].isin(train_epcs)].copy()\n",
    "test_df = df_engineered[df_engineered[\"epc_code\"].isin(test_epcs)].copy()\n",
    "\n",
    "print(f\"\\n[INFO] 데이터 분할 완료: Train={train_df.shape}, Test={test_df.shape}\")\n",
    "\n",
    "\n",
    "# --- 4. 스케일링을 수행합니다. ---\n",
    "# 스케일링 할 컬럼 목록: 이제 그룹 통계 피처들도 포함시킵니다.\n",
    "# (id 컬럼이나 sin/cos 변환된 컬럼은 보통 제외합니다.)\n",
    "cols_to_scale = [\n",
    "    'delta_t_sec', \n",
    "    'seq_length', \n",
    "    'n_unique_locations', \n",
    "    'delta_t_mean', \n",
    "    'delta_t_std', \n",
    "    'delta_t_max'\n",
    "]\n",
    "print(f\"[INFO] 스케일링 대상 컬럼: {cols_to_scale}\")\n",
    "\n",
    "# StandardScaler를 생성하고 '학습 데이터'에만 fit합니다.\n",
    "scaler = StandardScaler()\n",
    "print(f\"[INFO] 학습 데이터에 스케일러를 학습(fit)합니다.\")\n",
    "scaler.fit(train_df[cols_to_scale])\n",
    "\n",
    "# 학습된 스케일러로 Train, Test 데이터 모두를 transform합니다.\n",
    "print(\"[INFO] 학습된 스케일러로 Train/Test 데이터를 변환(transform)합니다.\")\n",
    "train_df[cols_to_scale] = scaler.transform(train_df[cols_to_scale])\n",
    "test_df[cols_to_scale] = scaler.transform(test_df[cols_to_scale])\n",
    "\n",
    "# 학습된 스케일러를 파일로 저장합니다. (모델 재사용을 위해 필수!)\n",
    "scaler_path = OUTDIR / \"if_scaler.joblib\" # Isolation Forest용 스케일러이므로 이름 변경\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"[OK] 스케일러 저장 완료: {scaler_path}\")\n",
    "\n",
    "\n",
    "# --- 5. 최종 데이터셋을 각각 저장합니다. ---\n",
    "# 이제 이 데이터는 예측 모델과 Isolation Forest 모두를 위한 준비가 되었습니다.\n",
    "save_table(train_df, OUTDIR / \"train_final_engineered.parquet\")\n",
    "save_table(test_df, OUTDIR / \"test_final_engineered.parquet\")\n",
    "\n",
    "print(\"\\n--- 스케일링 결과 확인 (Train Set) ---\")\n",
    "print(train_df[cols_to_scale].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36481d",
   "metadata": {},
   "source": [
    "# 7. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a276b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 전처리된 학습 데이터를 불러옵니다...\n",
      "[OK] preprocessed/train_final_engineered.parquet 로드 완료 (shape=(736188, 29))\n",
      "\n",
      "[INFO] Isolation Forest 모델을 훈련합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done   2 out of  32 | elapsed:    0.2s remaining:    2.3s\n",
      "[Parallel(n_jobs=32)]: Done  32 out of  32 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Isolation Forest 모델 훈련 완료.\n",
      "\n",
      "[SUCCESS] 훈련된 Isolation Forest 모델을 성공적으로 저장했습니다: preprocessed/iso_forest_model.joblib\n",
      "\n",
      "--- [참고] 훈련 데이터에 대한 간이 성능 테스트 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에서 '이상'으로 감지된 샘플 수: 369517\n",
      "훈련 데이터에서 '정상'으로 감지된 샘플 수: 366671\n",
      "(contamination='auto' 설정에 따라 약 36809개 내외의 이상치를 감지하는 것이 일반적입니다.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#      ★★ (신규 셀 7) Isolation Forest 모델 훈련 및 저장 ★★\n",
    "#     (이 셀은 전처리 마지막 셀(셀 6)이 완료된 후에 실행합니다)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report # 훈련 데이터에 대한 간이 테스트용\n",
    "\n",
    "# --- 1. 전처리된 최종 '학습' 데이터 로드 ---\n",
    "# Isolation Forest는 '정상' 데이터로만 훈련하는 것이 일반적이므로, train 셋만 필요합니다.\n",
    "print(\"[INFO] 전처리된 학습 데이터를 불러옵니다...\")\n",
    "try:\n",
    "    train_df = pd.read_parquet(OUTDIR / \"train_final_engineered.parquet\")\n",
    "    print(f\"[OK] {OUTDIR / 'train_final_engineered.parquet'} 로드 완료 (shape={train_df.shape})\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] 파일 로드 실패: {e}\")\n",
    "    # 파일이 없을 경우 스크립트 중단\n",
    "    raise SystemExit(\"[STOP] 훈련 데이터 파일을 찾을 수 없습니다. 셀 6을 먼저 실행하세요.\")\n",
    "\n",
    "    \n",
    "# --- 2. 모델 훈련에 사용할 특징(Features) 정의 ---\n",
    "# 전처리 단계에서 스케일링했던 모든 특징을 그대로 사용합니다.\n",
    "IF_FEATURES = [\n",
    "    # 원본 특징 (스케일링 되지 않은 ID, sin/cos)\n",
    "    'event_type_id', \n",
    "    'location_id_id', \n",
    "    'hour_sin', \n",
    "    'hour_cos',\n",
    "    # 스케일링 된 수치형 특징\n",
    "    'delta_t_sec',\n",
    "    'seq_length',           \n",
    "    'n_unique_locations',   \n",
    "    'delta_t_mean',         \n",
    "    'delta_t_std',          \n",
    "    'delta_t_max'\n",
    "]\n",
    "# 누락된 컬럼이 있는지 최종 확인\n",
    "missing_cols = [col for col in IF_FEATURES if col not in train_df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"필요한 피처 중 일부가 데이터에 없습니다: {missing_cols}\")\n",
    "    \n",
    "train_X_if = train_df[IF_FEATURES]\n",
    "\n",
    "\n",
    "# --- 3. Isolation Forest 모델 생성 및 훈련 ---\n",
    "print(\"\\n[INFO] Isolation Forest 모델을 훈련합니다...\")\n",
    "# 하이퍼파라미터 설정:\n",
    "#   - n_estimators: 사용할 나무(모델)의 개수. 많을수록 안정적이지만 훈련 시간이 길어짐.\n",
    "#   - contamination: 훈련 데이터에 포함될 것으로 예상되는 이상치의 비율. \n",
    "#     'auto'는 보수적인 기준이며, 약간의 이상치를 허용하려면 0.01 (1%) 등으로 설정 가능.\n",
    "#   - random_state: 재현성을 위해 난수 시드 고정.\n",
    "#   - n_jobs=-1: 컴퓨터의 모든 CPU 코어를 사용하여 훈련 속도 향상.\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination='auto', \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    verbose=1 # 훈련 과정을 보여줌\n",
    ")\n",
    "\n",
    "# 모델 훈련 (fit)\n",
    "iso_forest.fit(train_X_if)\n",
    "print(\"[OK] Isolation Forest 모델 훈련 완료.\")\n",
    "\n",
    "\n",
    "# --- 4. 훈련된 모델 저장 ---\n",
    "# LSTM 모델과 마찬가지로, 훈련된 Isolation Forest 모델을 저장하여\n",
    "# 나중에 평가 노트북에서 재사용합니다.\n",
    "model_path = OUTDIR / \"iso_forest_model.joblib\"\n",
    "joblib.dump(iso_forest, model_path)\n",
    "print(f\"\\n[SUCCESS] 훈련된 Isolation Forest 모델을 성공적으로 저장했습니다: {model_path}\")\n",
    "\n",
    "\n",
    "# --- 5. (선택 사항) 훈련 데이터 자체에 대한 간이 성능 확인 ---\n",
    "print(\"\\n--- [참고] 훈련 데이터에 대한 간이 성능 테스트 ---\")\n",
    "# 훈련 데이터에 대한 예측. -1이 이상치, 1이 정상\n",
    "predictions_on_train = iso_forest.predict(train_X_if)\n",
    "\n",
    "# -1(이상), 1(정상)을 1, 0으로 변환\n",
    "predictions_binary = np.where(predictions_on_train == -1, 1, 0)\n",
    "print(\"훈련 데이터에서 '이상'으로 감지된 샘플 수:\", np.sum(predictions_binary))\n",
    "print(\"훈련 데이터에서 '정상'으로 감지된 샘플 수:\", len(predictions_binary) - np.sum(predictions_binary))\n",
    "print(f\"(contamination='auto' 설정에 따라 약 {len(predictions_binary) * 0.05:.0f}개 내외의 이상치를 감지하는 것이 일반적입니다.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174eace",
   "metadata": {},
   "source": [
    "# 8. 앙상블 각각의 모델을 개별적으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46538900",
   "metadata": {},
   "source": [
    "# 성능 테스트"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
